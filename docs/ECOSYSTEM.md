# NovaRemote Ecosystem Guide

This guide covers companion-server distribution, desktop/web usage, and advanced external LLM capabilities.

## 1) Companion Server Distribution

NovaRemote works best when the companion server is publicly available and easy to install on any host.

Recommended track:

1. Keep `codex_remote` as a separate open-source repository.
2. Publish release tags with matching NovaRemote compatibility notes.
3. Publish a Docker image (`ghcr.io/maddwiz/codex-remote:latest`) for zero-friction installs.

### Fast install scripts (client repo)

From this NovaRemote repo:

- Linux: `npm run companion:install:linux`
- Windows (PowerShell): `npm run companion:install:windows`

What these scripts do:

- clone/update `codex_remote` (or use local sibling source if available)
- create Python virtualenv and install dependencies
- generate/preserve bearer token
- write runtime config in `~/.codexremote`
- create a start script for `uvicorn app.server:app`
- Linux script also writes a user `systemd` service unit (`codexremote.service`)

## 2) Desktop/Web Client Paths

NovaRemote already supports Expo web builds.

### Web development

- `npm run web`
- or clean dev session: `npm run desktop:web`

### Static web export

- `npm run web:export`
- output is generated by Expo web export for static hosting (Netlify/Vercel/Cloudflare Pages)

### Desktop wrappers

Two practical desktop tracks:

1. React Native Web only: host static export and install as browser app (PWA-like workflow).
2. Tauri wrapper: wrap exported web build into a desktop shell for local/native packaging.

## 3) External LLM Advanced Capabilities

NovaRemote now supports advanced test flows in the `LLMs` screen:

- vision input via image URL
- tool-calling with built-in local tools (OpenAI-compatible + Azure OpenAI chat endpoints)
- optional tool context payload for tool execution

Built-in tools currently include:

- `get_time`
- `explain_exit_code`
- `format_command_for_backend`
- `get_tool_context`

Notes:

- Real tool-calling is currently used for OpenAI-compatible and Azure OpenAI chat-style endpoints.
- For Anthropic/Gemini/Ollama profiles, vision/tool context is passed as structured prompt context for compatibility.

## 4) Next Ecosystem Milestones

1. Publish companion Docker image and add one-line quick start in README.
2. Add Linux distro packages (`deb`/`rpm`) for companion server.
3. Add desktop keychain-backed credential storage for web/desktop clients.
4. Add file/image picker to LLM screen for on-device vision input (not URL-only).
